{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flight_schemas import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName('flight_count_bp')\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flight_data = spark.read.csv('hdfs://namenode:9000/csv/all-years.csv', schema=flightSchema)\\\n",
    "#\t.withColumn('FL_DATE', to_date(to_timestamp('FL_DATE', 'M/d/yyyy h:mm:ss a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing upsert\n",
    "flight_data = spark.read.csv('hdfs://namenode:9000/csv/2022.csv', schema=flightSchema)\\\n",
    "\t.withColumn('FL_DATE', to_date(to_timestamp('FL_DATE', 'M/d/yyyy h:mm:ss a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant columns, dropping the rest\n",
    "flight_data = flight_data.select( 'year'\n",
    "                                , 'month'\n",
    "                                , 'fl_date'\n",
    "                                , 'op_unique_carrier'\n",
    "                                , 'origin_airport_id'\n",
    "                                , 'dest_airport_id'\n",
    "                                , 'dep_delay_new'\n",
    "                                , 'arr_delay_new'\n",
    "                                , 'cancelled'\n",
    "                                , 'diverted'\n",
    "                                , 'air_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataset from useless columns\n",
    "flight_data = flight_data.na.drop(subset=['year', 'origin_airport_id', 'dest_airport_id', 'fl_date'])\n",
    "flight_data = flight_data.fillna({'arr_delay_new': 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the fl_date of the flight with the highest delay for a given group\n",
    "windowSpec = Window.partitionBy('year'\n",
    "                            , 'month'\n",
    "                            , 'op_unique_carrier'\n",
    "                            , 'origin_airport_id'\n",
    "                            , 'dest_airport_id').orderBy(col('arr_delay_new').desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_delay_dates = flight_data.withColumn('rank'\n",
    "\t                                    , rank().over(windowSpec)\n",
    "                            ).filter(\n",
    "                                col('rank') == 1\n",
    "                            ).groupBy('year'\n",
    "                                    , 'month'\n",
    "                                    , 'op_unique_carrier'\n",
    "                                    , 'origin_airport_id'\n",
    "                                    , 'dest_airport_id'\n",
    "                            ).agg(round(max('arr_delay_new'), 2).alias('max_arr_delay')\n",
    "                                , first('fl_date').alias('max_arr_delay_fl_date')\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rest of the groupby\n",
    "flight_data = flight_data.groupBy('year'\n",
    "                                , 'month'\n",
    "                                , 'op_unique_carrier'\n",
    "                                , 'origin_airport_id'\n",
    "                                , 'dest_airport_id').agg( round(avg('arr_delay_new'), 2).alias('avg_arr_delay')\n",
    "                                                        , round(percentile_approx('arr_delay_new', 0.5), 2).alias('med_arr_delay')\n",
    "                                                        , round(avg(col('dep_delay_new') - col('arr_delay_new')), 2).alias('avg_time_recovered')\n",
    "                                                        , sum('diverted').alias('nr_diverted')\n",
    "                                                        , round(avg('air_time'), 2).alias('avg_airtime')\n",
    "                                                        , count('*').alias('flight_count')\n",
    "                                                        , sum('cancelled').alias('nr_cancelled'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the highest delay with the res of the group\n",
    "flight_data = arr_delay_dates.join( flight_data\n",
    "                                , on=['year', 'month', 'op_unique_carrier', 'origin_airport_id', 'dest_airport_id']\n",
    "                                , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the airports and airlines lookup tables\n",
    "airports = spark.read.csv('hdfs://namenode:9000/lookup_tables/airport_id.csv', schema=numIdSchema)\n",
    "carriers = spark.read.csv('hdfs://namenode:9000/lookup_tables/unique_carrier.csv', schema=StringIdSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data = flight_data.join(carriers.select('id', col('val').alias('airline'))\n",
    "                            , flight_data['op_unique_carrier'] == carriers['id']\n",
    "                            , how=\"left\"\n",
    "                        )\n",
    "\n",
    "flight_data = flight_data.join(airports.select('id', col('val').alias('origin_airport'))\n",
    "                            , flight_data['origin_airport_id'] == airports['id']\n",
    "                            , how=\"left\"\n",
    "                        )\n",
    "\n",
    "airports_alias = airports.alias('airports_alias')\n",
    "flight_data = flight_data.join(airports_alias.select('id', col('val').alias('dest_airport'))\n",
    "                            , flight_data['dest_airport_id'] == airports_alias['id']\n",
    "                            , how=\"left\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data = flight_data.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if Table exists\n",
    "if DeltaTable.isDeltaTable(spark, \"hdfs://namenode:9000/spark-warehouse/sample_flight_table\"):\n",
    "    # Perform the upsert operation\n",
    "    deltaDF = DeltaTable.forPath(spark, \"hdfs://namenode:9000/spark-warehouse/sample_flight_table\")\n",
    "    merge_condition = \"existing.year = upsert.year \\\n",
    "                    AND existing.month = upsert.month \\\n",
    "                    AND existing.op_unique_carrier = upsert.op_unique_carrier \\\n",
    "                    AND existing.origin_airport_id = upsert.origin_airport_id \\\n",
    "                    AND existing.dest_airport_id = upsert.dest_airport_id \"\n",
    "\n",
    "    deltaDF.alias('existing') \\\n",
    "        .merge(flight_data.alias('upsert'), merge_condition) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()  \n",
    "else:\n",
    "    # Create new delta table\n",
    "    flight_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sample_flight_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445659"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for testing\n",
    "deltaDF.toDF().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#445659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
