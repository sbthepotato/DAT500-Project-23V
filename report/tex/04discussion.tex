The observered results have shown that replacing the UDF's have the larger influence on the execution time. Reducing the allowed number of partitions to 48 also gave some improved results. While tuning the spark.sql.shuffle.partitions parameter we tested with 12, 32 and 64 partition numbers as well. Reducing it to 12 increased the execution time a bit, which seems logical since that would reduce the parallelism factor. Setting the value to 64 performed the same as it did with the default value of 200. The reason for this is our dataset size is too small and spark does self-optimization in many cases and in this case, it was only using 55-70 partitions, even when max value was set as 200. Hence, setting the value to 64 did not show any improvement. The performance for the values 32 and 48 were almost same but we chose 48 as we read that the optimal parallelism level is from 36 to 72 for a dataset with size as small as ours.\cite{tuning}

The different join techniques did not show any visible improvement in performance. One thing we observed was that Spark was automatically choosing to use broadcast join while executing the application, even when no join was explicitly mentioned. By this we concluded that explicitly mentioning join in the code had no impact on the application performance. In fact spark preferred the broadcast join so strongly for us that settings the configuration to prefer the sort merge join was not enough to get it to use it, we also had to set the broadcast join threshold to -1.

Overall it was difficult to show how much our optimisations actually mattered, this was most likely due to the dataset not being very large and the calculations being relatively simple. But if we had more data, we could experiment with different skew optimization techniques to see how they affect the performance.
