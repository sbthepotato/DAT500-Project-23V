\subsection{Use Case}
Did you ever experience being stranded in an airport because of flight delays or cancellations, and pondered on the possibility of being able to predict it with the help of more data? The use case that we chose for our project poses a solution for this problem. The U.S. Department of Transportation Statistics (DTS) tracks the on-time performance of domestic flights operated by large air carriers\cite{dataset}. Summary information on the number of on-time, delayed, cancelled, and diverted flights is published in the DTS's monthly Air Travel Consumer Report. We have taken our dataset from their official website. The dataset consists of the flight delay and cancellation information for the years from 2018 to 2022.

The goal of our use case is to analyse the trends in delays, to get an understanding of how the flight on-time performance varies in a particular time period. This analysis gives answer to various questions like, during which months in a year flights get most delayed, which day or week in a month has more possibility of flights getting delayed or cancelled, which flight carrier gets delayed more often compared to others and many more similar questions. This information can be useful for both a traveller and the airline carrier. Having access to this information a traveller can ensure that they do not miss any important meetings or events, while at the same time an airline carrier can know when it needs to increase their work force and use this opportunity to improve over their competitors.

\subsection{Background}
The goal of this project is to learn and get understanding of Hadoop and Spark Technologies which are frameworks for handling data intensive systems or nowadays commonly known as Big Data. Apache Hadoop is a collection of open-source modules and utilities intended to make the process of storing, managing and analysing big data easier while Apache Spark is an open-source data processing engine built for efficient, large-scale data analysis. Apache Spark can be run either standalone or as a software package on top of Apache Hadoop.

For our project we used Hadoop for converting our unstructured data which was in Text format to structured Comma Separated Values (.CSV) files. We used MapReduce in Hadoop to achieve this. Next, we used Spark to read this structured file from Hadoop Distributed File System (HDFS) and do the required processing on the dataset in line with the goal of our use case. One of the main aims of our project was to understand spark optimization techniques. We learnt about the five most common performance issues that can be encountered while executing a spark application, those issues are spill, skew, shuffle, storage and serialisation. From those five we selected the 3 which we believed would help the most at making our application run faster. 

\subsection{optimisations}
Among these, we observed that our spark application can be improved by solving problems related to Spill and Skew (Inter-related in our case), Shuffle and Serialization.

\emph{Spill} is the term used to refer to the act of moving an RDD from RAM to disk, and later back into RAM again. This occurs when a given partition is simply too large to fit into RAM. In this case, Spark is forced into potentially expensive disk reads and writes to free up local RAM. All of this just to avoid the dreaded OOM (Out of Memory) Error. One of the reasons of Spill could be the Skew in the dataset. Hence, skew can induce spill. We observed that there was skew in our dataset, there was a lot more information on some carriers compared to very little information on other carriers. We also observed in the Spark UI that there was Spill in some of the stages of execution of the application processes.

\emph{Shuffle} is a side effect of wide transformation caused by operations like join() and group by() on the data. Shuffling refers to transfer of data over the network which adds as overhead time to the actual processing time. Since we are using multiple of these operations in our application, we chose to attempt to minimise this problem.

\emph{Serialization} occurs when the code has to be serialised, this means that the code is sent to the executors and then de-serialised before it can be executed. Python code will take an even harder hit due to it having to be pickled and the spark must initiate an instance of the python interpreter in every single executer. meanwhile Spark SQL and Dataframe instructions are compact and optimized for distribution instructiosn from the driver to each executor. Our code was using a few User Defined Functions (UDF's) so this was the last of our 3 optimisations.

